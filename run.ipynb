{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjkGsD8mKrij",
        "outputId": "060ba1b1-7636-491d-b686-2428947badbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csntax-gnn'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 50 (delta 22), reused 12 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (50/50), 19.77 KiB | 19.77 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/igorsterner/csntax-gnn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd csntax-gnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP2nlr8uKvmx",
        "outputId": "2ecedea1-68f8-41a6-ccc3-7fca84750d85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/csntax-gnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vONm-weuLCOc",
        "outputId": "20f19e90-b75b-475d-be63-ccaece511c9e",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.1.post1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.1.post1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/igorsterner/acs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlPEOHj0LF_A",
        "outputId": "2baded71-3a1b-4b31-df4d-bbff58a1d98a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'acs'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 84 (delta 22), reused 36 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (84/84), 36.08 MiB | 23.13 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvX10nJTMl9m",
        "outputId": "4a927135-2d39-4ae7-c602-561a0cee6410",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `acs` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `acs`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-compute all the required data (translations, parses & alignment), and then train the model. Skip the next two cells by downloading the data and trained models from: https://www.dropbox.com/scl/fo/ttwy03ga38di3r21a9d7h/ADD4X7NTUbRnufp0dTkrLTk?rlkey=9678qpegrkresuag08m8clp3c&st=ske182n0&dl=0. If you do so, make sure you have the data in `data/preprocessed/` and pytorch models in `data/models/`"
      ],
      "metadata": {
        "id": "eXfj3oWKsSxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets huggingface_hub fsspec --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ua9bIPLv600",
        "outputId": "b8126c6d-b710-4a01-d9ec-a56f5df6639b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/494.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/494.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 de --lang2 en --split train\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 de --lang2 en --split validation\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 de --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 da --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 es --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 fr --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 it --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 id --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 nl --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 sv --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 tr --lang2 en --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 tr --lang2 de --split test\n",
        "!PYTHONPATH=$(pwd) python csntax/data.py --lang1 zh --lang2 en --split test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVfJc8nzMH2y",
        "outputId": "d15223bc-1de2-445c-b7b8-0b7a5f03ba2e",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-18 08:56:43.845703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752829003.866341    4737 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752829003.872659    4737 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=train, lang=de-en\n",
            "README.md: 6.74kB [00:00, 28.8MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.32M/2.32M [00:00<00:00, 6.25MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 597k/597k [00:00<00:00, 24.7MB/s]\n",
            "test-00000-of-00001.parquet: 100% 317k/317k [00:00<00:00, 55.0MB/s]\n",
            "Generating train split: 7501 examples [00:00, 232601.71 examples/s]\n",
            "Generating validation split: 1932 examples [00:00, 356962.04 examples/s]\n",
            "Generating test split: 1000 examples [00:00, 296166.08 examples/s]\n",
            "Working with 15002 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 749/749 [00:00<00:00, 7.89MB/s]\n",
            "model.safetensors: 100% 11.8G/11.8G [01:00<00:00, 193MB/s]\n",
            "generation_config.json: 100% 142/142 [00:00<00:00, 1.59MB/s]\n",
            "spiece.model: 100% 4.43M/4.43M [00:00<00:00, 18.3MB/s]\n",
            "added_tokens.json: 100% 4.00/4.00 [00:00<00:00, 48.2kB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 5.25MB/s]\n",
            "tokenizer_config.json: 100% 830/830 [00:00<00:00, 10.6MB/s]\n",
            "Madlad translate to de: 100% 469/469 [21:13<00:00,  2.72s/it]\n",
            "Madlad translate to en: 100% 469/469 [19:02<00:00,  2.44s/it]\n",
            "Parsing\n",
            "2025-07-18 09:39:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 28.9MB/s]        \n",
            "2025-07-18 09:39:11 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/tokenize/combined.pt: 100% 655k/655k [00:00<00:00, 5.57MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/mwt/combined.pt: 100% 547k/547k [00:00<00:00, 4.81MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/pos/combined_charlm.pt: 100% 38.0M/38.0M [00:00<00:00, 102MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100% 40.6M/40.6M [00:00<00:00, 101MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/constituency/spmrl_charlm.pt: 100% 129M/129M [00:00<00:00, 146MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/depparse/combined_charlm.pt: 100% 139M/139M [00:00<00:00, 150MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/sentiment/sb10k_charlm.pt: 100% 77.6M/77.6M [00:00<00:00, 123MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/ner/germeval2014.pt: 100% 50.8M/50.8M [00:00<00:00, 116MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 145MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/forward_charlm/newswiki.pt: 100% 20.0M/20.0M [00:00<00:00, 74.5MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/pretrain/fasttextwiki.pt: 100% 307M/307M [00:01<00:00, 161MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.10.0/models/backward_charlm/newswiki.pt: 100% 20.0M/20.0M [00:00<00:00, 70.6MB/s]\n",
            "2025-07-18 09:39:23 INFO: Loading these models for language: de (German):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | spmrl_charlm      |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | sb10k_charlm      |\n",
            "| ner          | germeval2014      |\n",
            "====================================\n",
            "\n",
            "2025-07-18 09:39:23 INFO: Using device: cuda\n",
            "2025-07-18 09:39:23 INFO: Loading: tokenize\n",
            "2025-07-18 09:39:25 INFO: Loading: mwt\n",
            "2025-07-18 09:39:25 INFO: Loading: pos\n",
            "2025-07-18 09:39:27 INFO: Loading: lemma\n",
            "2025-07-18 09:39:35 INFO: Loading: constituency\n",
            "2025-07-18 09:39:36 INFO: Loading: depparse\n",
            "2025-07-18 09:39:36 INFO: Loading: sentiment\n",
            "2025-07-18 09:39:37 INFO: Loading: ner\n",
            "2025-07-18 09:39:41 INFO: Done loading processors!\n",
            "2025-07-18 09:43:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 29.1MB/s]        \n",
            "2025-07-18 09:43:04 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/tokenize/combined.pt: 100% 651k/651k [00:00<00:00, 5.44MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/mwt/combined.pt: 100% 509k/509k [00:00<00:00, 4.61MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/pos/combined_charlm.pt: 100% 38.6M/38.6M [00:00<00:00, 97.9MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100% 4.27M/4.27M [00:00<00:00, 24.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/constituency/ptb3-revised_charlm.pt: 100% 113M/113M [00:00<00:00, 145MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/depparse/combined_charlm.pt: 100% 145M/145M [00:01<00:00, 145MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/sentiment/sstplus_charlm.pt: 100% 80.4M/80.4M [00:00<00:00, 132MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/ner/ontonotes-ww-multi_charlm.pt: 100% 65.7M/65.7M [00:00<00:00, 115MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 134MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/backward_charlm/1billion.pt: 100% 22.7M/22.7M [00:00<00:00, 82.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/forward_charlm/1billion.pt: 100% 22.7M/22.7M [00:00<00:00, 78.1MB/s]\n",
            "2025-07-18 09:43:13 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 09:43:13 INFO: Using device: cuda\n",
            "2025-07-18 09:43:13 INFO: Loading: tokenize\n",
            "2025-07-18 09:43:13 INFO: Loading: mwt\n",
            "2025-07-18 09:43:13 INFO: Loading: pos\n",
            "2025-07-18 09:43:14 INFO: Loading: lemma\n",
            "2025-07-18 09:43:15 INFO: Loading: constituency\n",
            "2025-07-18 09:43:16 INFO: Loading: depparse\n",
            "2025-07-18 09:43:16 INFO: Loading: sentiment\n",
            "2025-07-18 09:43:16 INFO: Loading: ner\n",
            "2025-07-18 09:43:18 INFO: Done loading processors!\n",
            "Aligning\n",
            "config.json: 1.19kB [00:00, 6.68MB/s]\n",
            "pytorch_model.bin: 100% 1.09G/1.09G [00:13<00:00, 81.7MB/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.bias', 'guide_layer.linear.weight', 'guide_layer.linear2.weight', 'cls.predictions.transform.dense.weight', 'guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'psi_cls.transform.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "vocab.txt: 996kB [00:00, 17.0MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 734kB/s]\n",
            "tokenizer_config.json: 100% 40.0/40.0 [00:00<00:00, 231kB/s]\n",
            "100% 469/469 [00:45<00:00, 10.39it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.bias', 'guide_layer.linear.weight', 'guide_layer.linear2.weight', 'cls.predictions.transform.dense.weight', 'guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'psi_cls.transform.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 469/469 [00:47<00:00,  9.77it/s]\n",
            "2025-07-18 09:49:47.969630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752832187.991133   17886 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752832187.997679   17886 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=validation, lang=de-en\n",
            "Working with 3864 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Madlad translate to de: 100% 121/121 [05:15<00:00,  2.61s/it]\n",
            "Madlad translate to en: 100% 121/121 [04:36<00:00,  2.29s/it]\n",
            "Parsing\n",
            "2025-07-18 10:00:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.4MB/s]        \n",
            "2025-07-18 10:00:47 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:00:49 INFO: Loading these models for language: de (German):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | spmrl_charlm      |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | sb10k_charlm      |\n",
            "| ner          | germeval2014      |\n",
            "====================================\n",
            "\n",
            "2025-07-18 10:00:49 INFO: Using device: cuda\n",
            "2025-07-18 10:00:49 INFO: Loading: tokenize\n",
            "2025-07-18 10:00:50 INFO: Loading: mwt\n",
            "2025-07-18 10:00:50 INFO: Loading: pos\n",
            "2025-07-18 10:00:52 INFO: Loading: lemma\n",
            "2025-07-18 10:01:00 INFO: Loading: constituency\n",
            "2025-07-18 10:01:00 INFO: Loading: depparse\n",
            "2025-07-18 10:01:01 INFO: Loading: sentiment\n",
            "2025-07-18 10:01:01 INFO: Loading: ner\n",
            "2025-07-18 10:01:05 INFO: Done loading processors!\n",
            "2025-07-18 10:01:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.7MB/s]        \n",
            "2025-07-18 10:01:58 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:01:59 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:01:59 INFO: Using device: cuda\n",
            "2025-07-18 10:01:59 INFO: Loading: tokenize\n",
            "2025-07-18 10:01:59 INFO: Loading: mwt\n",
            "2025-07-18 10:01:59 INFO: Loading: pos\n",
            "2025-07-18 10:02:01 INFO: Loading: lemma\n",
            "2025-07-18 10:02:01 INFO: Loading: constituency\n",
            "2025-07-18 10:02:02 INFO: Loading: depparse\n",
            "2025-07-18 10:02:02 INFO: Loading: sentiment\n",
            "2025-07-18 10:02:03 INFO: Loading: ner\n",
            "2025-07-18 10:02:05 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear2.weight', 'cls.predictions.transform.dense.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.weight', 'cls.predictions.bias', 'guide_layer.linear.weight', 'guide_layer.linear.bias', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.bias', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 121/121 [00:11<00:00, 10.44it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear2.weight', 'cls.predictions.transform.dense.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.weight', 'cls.predictions.bias', 'guide_layer.linear.weight', 'guide_layer.linear.bias', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.bias', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 121/121 [00:12<00:00,  9.91it/s]\n",
            "2025-07-18 10:03:49.209373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752833029.230386   21510 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752833029.236813   21510 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=de-en\n",
            "Working with 2000 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to de: 100% 63/63 [02:49<00:00,  2.68s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 63/63 [02:27<00:00,  2.35s/it]\n",
            "Parsing\n",
            "2025-07-18 10:10:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.2MB/s]        \n",
            "2025-07-18 10:10:13 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:10:15 INFO: Loading these models for language: de (German):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | spmrl_charlm      |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | sb10k_charlm      |\n",
            "| ner          | germeval2014      |\n",
            "====================================\n",
            "\n",
            "2025-07-18 10:10:15 INFO: Using device: cuda\n",
            "2025-07-18 10:10:15 INFO: Loading: tokenize\n",
            "2025-07-18 10:10:16 INFO: Loading: mwt\n",
            "2025-07-18 10:10:16 INFO: Loading: pos\n",
            "2025-07-18 10:10:17 INFO: Loading: lemma\n",
            "2025-07-18 10:10:25 INFO: Loading: constituency\n",
            "2025-07-18 10:10:26 INFO: Loading: depparse\n",
            "2025-07-18 10:10:27 INFO: Loading: sentiment\n",
            "2025-07-18 10:10:27 INFO: Loading: ner\n",
            "2025-07-18 10:10:31 INFO: Done loading processors!\n",
            "2025-07-18 10:10:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 29.9MB/s]        \n",
            "2025-07-18 10:10:59 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:11:00 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:11:00 INFO: Using device: cuda\n",
            "2025-07-18 10:11:00 INFO: Loading: tokenize\n",
            "2025-07-18 10:11:00 INFO: Loading: mwt\n",
            "2025-07-18 10:11:00 INFO: Loading: pos\n",
            "2025-07-18 10:11:02 INFO: Loading: lemma\n",
            "2025-07-18 10:11:02 INFO: Loading: constituency\n",
            "2025-07-18 10:11:03 INFO: Loading: depparse\n",
            "2025-07-18 10:11:03 INFO: Loading: sentiment\n",
            "2025-07-18 10:11:04 INFO: Loading: ner\n",
            "2025-07-18 10:11:06 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.transform.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.weight', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'psi_cls.transform.bias', 'psi_cls.bias', 'guide_layer.linear.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:06<00:00, 10.35it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.transform.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.weight', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'psi_cls.transform.bias', 'psi_cls.bias', 'guide_layer.linear.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:06<00:00,  9.75it/s]\n",
            "2025-07-18 10:12:05.177539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752833525.198899   23666 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752833525.205414   23666 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=es-en\n",
            "test-00000-of-00001.parquet: 100% 244k/244k [00:00<00:00, 2.63MB/s]\n",
            "Generating test split: 860 examples [00:00, 126529.45 examples/s]\n",
            "Working with 1720 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Madlad translate to es: 100% 54/54 [02:18<00:00,  2.56s/it]\n",
            "Madlad translate to en: 100% 54/54 [02:16<00:00,  2.52s/it]\n",
            "Parsing\n",
            "2025-07-18 10:17:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.8MB/s]        \n",
            "2025-07-18 10:17:45 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/tokenize/combined.pt: 100% 664k/664k [00:00<00:00, 5.66MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/mwt/combined.pt: 100% 634k/634k [00:00<00:00, 5.36MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pos/combined_charlm.pt: 100% 38.0M/38.0M [00:00<00:00, 99.5MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100% 6.87M/6.87M [00:00<00:00, 36.0MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/constituency/combined_charlm.pt: 100% 113M/113M [00:00<00:00, 140MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/depparse/combined_charlm.pt: 100% 142M/142M [00:00<00:00, 148MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/sentiment/tass2020_charlm.pt: 100% 74.0M/74.0M [00:00<00:00, 131MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/ner/conll02.pt: 100% 67.0M/67.0M [00:00<00:00, 123MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/forward_charlm/newswiki.pt: 100% 20.0M/20.0M [00:00<00:00, 71.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/backward_charlm/newswiki.pt: 100% 20.0M/20.0M [00:00<00:00, 67.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 141MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pretrain/fasttextwiki.pt: 100% 123M/123M [00:00<00:00, 148MB/s]\n",
            "2025-07-18 10:17:55 INFO: Loading these models for language: es (Spanish):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | combined_charlm   |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | tass2020_charlm   |\n",
            "| ner          | conll02           |\n",
            "====================================\n",
            "\n",
            "2025-07-18 10:17:55 INFO: Using device: cuda\n",
            "2025-07-18 10:17:55 INFO: Loading: tokenize\n",
            "2025-07-18 10:17:56 INFO: Loading: mwt\n",
            "2025-07-18 10:17:56 INFO: Loading: pos\n",
            "2025-07-18 10:17:58 INFO: Loading: lemma\n",
            "2025-07-18 10:17:59 INFO: Loading: constituency\n",
            "2025-07-18 10:17:59 INFO: Loading: depparse\n",
            "2025-07-18 10:18:00 INFO: Loading: sentiment\n",
            "2025-07-18 10:18:00 INFO: Loading: ner\n",
            "2025-07-18 10:18:02 INFO: Done loading processors!\n",
            "2025-07-18 10:18:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.4MB/s]        \n",
            "2025-07-18 10:18:35 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:18:36 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:18:36 INFO: Using device: cuda\n",
            "2025-07-18 10:18:36 INFO: Loading: tokenize\n",
            "2025-07-18 10:18:36 INFO: Loading: mwt\n",
            "2025-07-18 10:18:36 INFO: Loading: pos\n",
            "2025-07-18 10:18:38 INFO: Loading: lemma\n",
            "2025-07-18 10:18:39 INFO: Loading: constituency\n",
            "2025-07-18 10:18:39 INFO: Loading: depparse\n",
            "2025-07-18 10:18:40 INFO: Loading: sentiment\n",
            "2025-07-18 10:18:40 INFO: Loading: ner\n",
            "2025-07-18 10:18:42 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'psi_cls.transform.bias', 'psi_cls.bias', 'cls.predictions.transform.dense.bias', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'psi_cls.transform.weight', 'guide_layer.linear2.weight', 'cls.predictions.bias', 'guide_layer.linear.weight', 'guide_layer.linear.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 54/54 [00:05<00:00, 10.80it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'psi_cls.transform.bias', 'psi_cls.bias', 'cls.predictions.transform.dense.bias', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'psi_cls.transform.weight', 'guide_layer.linear2.weight', 'cls.predictions.bias', 'guide_layer.linear.weight', 'guide_layer.linear.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 54/54 [00:05<00:00, 10.42it/s]\n",
            "2025-07-18 10:19:32.972258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752833972.993646   25583 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752833973.000214   25583 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=fr-en\n",
            "test-00000-of-00001.parquet: 100% 250k/250k [00:00<00:00, 2.91MB/s]\n",
            "Generating test split: 1000 examples [00:00, 164766.81 examples/s]\n",
            "Working with 2000 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to fr: 100% 63/63 [02:30<00:00,  2.39s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 63/63 [02:16<00:00,  2.16s/it]\n",
            "Parsing\n",
            "2025-07-18 10:25:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.5MB/s]        \n",
            "2025-07-18 10:25:25 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/tokenize/combined.pt: 100% 665k/665k [00:00<00:00, 5.68MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/mwt/combined.pt: 100% 545k/545k [00:00<00:00, 4.86MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/pos/combined_charlm.pt: 100% 34.4M/34.4M [00:00<00:00, 99.2MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100% 5.41M/5.41M [00:00<00:00, 29.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/depparse/combined_charlm.pt: 100% 149M/149M [00:00<00:00, 153MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/ner/wikinergold_charlm.pt: 100% 62.9M/62.9M [00:00<00:00, 119MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 141MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/backward_charlm/newswiki.pt: 100% 20.1M/20.1M [00:00<00:00, 74.3MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/forward_charlm/newswiki.pt: 100% 20.1M/20.1M [00:00<00:00, 75.8MB/s]\n",
            "2025-07-18 10:25:32 INFO: Loading these models for language: fr (French):\n",
            "==================================\n",
            "| Processor | Package            |\n",
            "----------------------------------\n",
            "| tokenize  | combined           |\n",
            "| mwt       | combined           |\n",
            "| pos       | combined_charlm    |\n",
            "| lemma     | combined_nocharlm  |\n",
            "| depparse  | combined_charlm    |\n",
            "| ner       | wikinergold_charlm |\n",
            "==================================\n",
            "\n",
            "2025-07-18 10:25:32 INFO: Using device: cuda\n",
            "2025-07-18 10:25:32 INFO: Loading: tokenize\n",
            "2025-07-18 10:25:32 INFO: Loading: mwt\n",
            "2025-07-18 10:25:32 INFO: Loading: pos\n",
            "2025-07-18 10:25:34 INFO: Loading: lemma\n",
            "2025-07-18 10:25:35 INFO: Loading: depparse\n",
            "2025-07-18 10:25:35 INFO: Loading: ner\n",
            "2025-07-18 10:25:37 INFO: Done loading processors!\n",
            "2025-07-18 10:25:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 33.9MB/s]        \n",
            "2025-07-18 10:25:51 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:25:52 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:25:52 INFO: Using device: cuda\n",
            "2025-07-18 10:25:52 INFO: Loading: tokenize\n",
            "2025-07-18 10:25:52 INFO: Loading: mwt\n",
            "2025-07-18 10:25:52 INFO: Loading: pos\n",
            "2025-07-18 10:25:54 INFO: Loading: lemma\n",
            "2025-07-18 10:25:55 INFO: Loading: constituency\n",
            "2025-07-18 10:25:55 INFO: Loading: depparse\n",
            "2025-07-18 10:25:56 INFO: Loading: sentiment\n",
            "2025-07-18 10:25:56 INFO: Loading: ner\n",
            "2025-07-18 10:25:58 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.decoder.weight', 'psi_cls.transform.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'cls.predictions.decoder.bias', 'guide_layer.linear2.weight', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.weight', 'guide_layer.linear2.bias', 'psi_cls.transform.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 11.79it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.decoder.weight', 'psi_cls.transform.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'cls.predictions.decoder.bias', 'guide_layer.linear2.weight', 'psi_cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.weight', 'guide_layer.linear2.bias', 'psi_cls.transform.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 11.05it/s]\n",
            "2025-07-18 10:26:50.691888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752834410.713609   27417 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752834410.720243   27417 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=it-en\n",
            "test-00000-of-00001.parquet: 100% 287k/287k [00:00<00:00, 3.31MB/s]\n",
            "Generating test split: 1000 examples [00:00, 156995.96 examples/s]\n",
            "Working with 2000 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to it: 100% 63/63 [02:59<00:00,  2.85s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 63/63 [02:56<00:00,  2.80s/it]\n",
            "Parsing\n",
            "2025-07-18 10:33:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.4MB/s]        \n",
            "2025-07-18 10:33:53 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/tokenize/combined.pt: 100% 637k/637k [00:00<00:00, 4.77MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/mwt/combined.pt: 100% 1.17M/1.17M [00:00<00:00, 7.81MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/pos/combined_charlm.pt: 100% 35.0M/35.0M [00:00<00:00, 86.8MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100% 5.71M/5.71M [00:00<00:00, 23.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/constituency/vit_charlm.pt: 100% 111M/111M [00:00<00:00, 127MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/depparse/combined_charlm.pt: 100% 149M/149M [00:01<00:00, 136MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/ner/fbk.pt: 100% 55.0M/55.0M [00:00<00:00, 63.8MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 125MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/forward_charlm/conll17.pt: 100% 22.6M/22.6M [00:00<00:00, 72.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/backward_charlm/conll17.pt: 100% 22.6M/22.6M [00:00<00:00, 72.3MB/s]\n",
            "2025-07-18 10:34:01 INFO: Loading these models for language: it (Italian):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | vit_charlm        |\n",
            "| depparse     | combined_charlm   |\n",
            "| ner          | fbk               |\n",
            "====================================\n",
            "\n",
            "2025-07-18 10:34:01 INFO: Using device: cuda\n",
            "2025-07-18 10:34:01 INFO: Loading: tokenize\n",
            "2025-07-18 10:34:02 INFO: Loading: mwt\n",
            "2025-07-18 10:34:02 INFO: Loading: pos\n",
            "2025-07-18 10:34:04 INFO: Loading: lemma\n",
            "2025-07-18 10:34:05 INFO: Loading: constituency\n",
            "2025-07-18 10:34:05 INFO: Loading: depparse\n",
            "2025-07-18 10:34:06 INFO: Loading: ner\n",
            "2025-07-18 10:34:08 INFO: Done loading processors!\n",
            "2025-07-18 10:34:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.7MB/s]        \n",
            "2025-07-18 10:34:40 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:34:41 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:34:41 INFO: Using device: cuda\n",
            "2025-07-18 10:34:41 INFO: Loading: tokenize\n",
            "2025-07-18 10:34:41 INFO: Loading: mwt\n",
            "2025-07-18 10:34:42 INFO: Loading: pos\n",
            "2025-07-18 10:34:43 INFO: Loading: lemma\n",
            "2025-07-18 10:34:44 INFO: Loading: constituency\n",
            "2025-07-18 10:34:44 INFO: Loading: depparse\n",
            "2025-07-18 10:34:45 INFO: Loading: sentiment\n",
            "2025-07-18 10:34:45 INFO: Loading: ner\n",
            "2025-07-18 10:34:47 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.bias', 'guide_layer.linear.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'psi_cls.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.LayerNorm.bias', 'guide_layer.linear2.bias', 'guide_layer.linear2.weight', 'psi_cls.transform.weight', 'psi_cls.transform.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 10.91it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['psi_cls.bias', 'guide_layer.linear.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'psi_cls.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.LayerNorm.bias', 'guide_layer.linear2.bias', 'guide_layer.linear2.weight', 'psi_cls.transform.weight', 'psi_cls.transform.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 10.53it/s]\n",
            "2025-07-18 10:35:43.811297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752834943.832850   29641 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752834943.839394   29641 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=id-en\n",
            "test-00000-of-00001.parquet: 100% 286k/286k [00:00<00:00, 2.97MB/s]\n",
            "Generating test split: 1000 examples [00:00, 161393.87 examples/s]\n",
            "Working with 2000 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to id: 100% 63/63 [02:29<00:00,  2.38s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 63/63 [02:57<00:00,  2.82s/it]\n",
            "Parsing\n",
            "2025-07-18 10:42:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.8MB/s]        \n",
            "2025-07-18 10:42:18 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/tokenize/gsd.pt: 100% 659k/659k [00:00<00:00, 5.69MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/mwt/gsd.pt: 100% 500k/500k [00:00<00:00, 4.71MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/pos/gsd_charlm.pt: 100% 32.5M/32.5M [00:00<00:00, 77.4MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/lemma/gsd_nocharlm.pt: 100% 3.44M/3.44M [00:00<00:00, 19.4MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/constituency/icon_charlm.pt: 100% 107M/107M [00:01<00:00, 87.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/depparse/gsd_charlm.pt: 100% 140M/140M [00:01<00:00, 78.8MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/forward_charlm/oscar2023.pt: 100% 22.3M/22.3M [00:00<00:00, 65.6MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:01<00:00, 83.2MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-id/resolve/v1.10.0/models/backward_charlm/oscar2023.pt: 100% 22.3M/22.3M [00:00<00:00, 66.1MB/s]\n",
            "2025-07-18 10:42:26 INFO: Loading these models for language: id (Indonesian):\n",
            "===============================\n",
            "| Processor    | Package      |\n",
            "-------------------------------\n",
            "| tokenize     | gsd          |\n",
            "| mwt          | gsd          |\n",
            "| pos          | gsd_charlm   |\n",
            "| lemma        | gsd_nocharlm |\n",
            "| constituency | icon_charlm  |\n",
            "| depparse     | gsd_charlm   |\n",
            "===============================\n",
            "\n",
            "2025-07-18 10:42:26 INFO: Using device: cuda\n",
            "2025-07-18 10:42:26 INFO: Loading: tokenize\n",
            "2025-07-18 10:42:27 INFO: Loading: mwt\n",
            "2025-07-18 10:42:27 INFO: Loading: pos\n",
            "2025-07-18 10:42:29 INFO: Loading: lemma\n",
            "2025-07-18 10:42:29 INFO: Loading: constituency\n",
            "2025-07-18 10:42:30 INFO: Loading: depparse\n",
            "2025-07-18 10:42:30 INFO: Done loading processors!\n",
            "2025-07-18 10:42:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.2MB/s]        \n",
            "2025-07-18 10:42:55 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:42:56 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:42:56 INFO: Using device: cuda\n",
            "2025-07-18 10:42:56 INFO: Loading: tokenize\n",
            "2025-07-18 10:42:56 INFO: Loading: mwt\n",
            "2025-07-18 10:42:56 INFO: Loading: pos\n",
            "2025-07-18 10:42:58 INFO: Loading: lemma\n",
            "2025-07-18 10:42:58 INFO: Loading: constituency\n",
            "2025-07-18 10:42:59 INFO: Loading: depparse\n",
            "2025-07-18 10:42:59 INFO: Loading: sentiment\n",
            "2025-07-18 10:43:00 INFO: Loading: ner\n",
            "2025-07-18 10:43:02 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear2.weight', 'psi_cls.transform.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'psi_cls.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'cls.predictions.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 10.71it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear2.weight', 'psi_cls.transform.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'guide_layer.linear2.bias', 'cls.predictions.decoder.weight', 'psi_cls.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'cls.predictions.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 10.52it/s]\n",
            "2025-07-18 10:43:59.686510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752835439.707727   31709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752835439.714257   31709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=nl-en\n",
            "test-00000-of-00001.parquet: 100% 291k/291k [00:00<00:00, 3.20MB/s]\n",
            "Generating test split: 1000 examples [00:00, 157502.97 examples/s]\n",
            "Working with 2000 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to nl: 100% 63/63 [02:51<00:00,  2.73s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 63/63 [02:41<00:00,  2.56s/it]\n",
            "Parsing\n",
            "2025-07-18 10:50:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.1MB/s]        \n",
            "2025-07-18 10:50:40 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/tokenize/alpino.pt: 100% 634k/634k [00:00<00:00, 3.03MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/mwt/alpino.pt: 100% 1.50k/1.50k [00:00<00:00, 11.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/pos/alpino_charlm.pt: 100% 35.5M/35.5M [00:00<00:00, 225MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/lemma/alpino_nocharlm.pt: 100% 3.82M/3.82M [00:00<00:00, 49.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/depparse/alpino_charlm.pt: 100% 134M/134M [00:00<00:00, 290MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/ner/conll02.pt: 100% 66.3M/66.3M [00:00<00:00, 247MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/pretrain/fasttextwiki.pt: 100% 123M/123M [00:00<00:00, 299MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/backward_charlm/ccwiki.pt: 100% 20.4M/20.4M [00:00<00:00, 170MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/forward_charlm/ccwiki.pt: 100% 20.4M/20.4M [00:00<00:00, 157MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-nl/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 312MB/s] \n",
            "2025-07-18 10:50:45 INFO: Loading these models for language: nl (Dutch):\n",
            "===============================\n",
            "| Processor | Package         |\n",
            "-------------------------------\n",
            "| tokenize  | alpino          |\n",
            "| mwt       | alpino          |\n",
            "| pos       | alpino_charlm   |\n",
            "| lemma     | alpino_nocharlm |\n",
            "| depparse  | alpino_charlm   |\n",
            "| ner       | conll02         |\n",
            "===============================\n",
            "\n",
            "2025-07-18 10:50:45 INFO: Using device: cuda\n",
            "2025-07-18 10:50:45 INFO: Loading: tokenize\n",
            "2025-07-18 10:50:45 INFO: Loading: mwt\n",
            "2025-07-18 10:50:45 INFO: Loading: pos\n",
            "2025-07-18 10:50:47 INFO: Loading: lemma\n",
            "2025-07-18 10:50:48 INFO: Loading: depparse\n",
            "2025-07-18 10:50:48 INFO: Loading: ner\n",
            "2025-07-18 10:50:50 INFO: Done loading processors!\n",
            "2025-07-18 10:51:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.5MB/s]        \n",
            "2025-07-18 10:51:05 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:51:07 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:51:07 INFO: Using device: cuda\n",
            "2025-07-18 10:51:07 INFO: Loading: tokenize\n",
            "2025-07-18 10:51:07 INFO: Loading: mwt\n",
            "2025-07-18 10:51:07 INFO: Loading: pos\n",
            "2025-07-18 10:51:08 INFO: Loading: lemma\n",
            "2025-07-18 10:51:09 INFO: Loading: constituency\n",
            "2025-07-18 10:51:09 INFO: Loading: depparse\n",
            "2025-07-18 10:51:10 INFO: Loading: sentiment\n",
            "2025-07-18 10:51:10 INFO: Loading: ner\n",
            "2025-07-18 10:51:12 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear2.bias', 'psi_cls.bias', 'guide_layer.linear2.weight', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.weight', 'psi_cls.transform.weight', 'cls.predictions.decoder.weight', 'psi_cls.decoder.bias', 'psi_cls.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'psi_cls.transform.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:05<00:00, 10.91it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear2.bias', 'psi_cls.bias', 'guide_layer.linear2.weight', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.weight', 'psi_cls.transform.weight', 'cls.predictions.decoder.weight', 'psi_cls.decoder.bias', 'psi_cls.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.bias', 'psi_cls.transform.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 63/63 [00:06<00:00, 10.32it/s]\n",
            "2025-07-18 10:52:07.835274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752835927.856658   33749 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752835927.863254   33749 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=sv-en\n",
            "test-00000-of-00001.parquet: 100% 228k/228k [00:00<00:00, 1.53MB/s]\n",
            "Generating test split: 760 examples [00:00, 125553.23 examples/s]\n",
            "Working with 1520 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Madlad translate to sv: 100% 48/48 [01:58<00:00,  2.48s/it]\n",
            "Madlad translate to en: 100% 48/48 [01:50<00:00,  2.29s/it]\n",
            "Parsing\n",
            "2025-07-18 10:57:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.5MB/s]        \n",
            "2025-07-18 10:57:03 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/tokenize/talbanken.pt: 100% 629k/629k [00:00<00:00, 5.39MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/pos/talbanken_charlm.pt: 100% 33.7M/33.7M [00:00<00:00, 72.9MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/lemma/talbanken_nocharlm.pt: 100% 2.85M/2.85M [00:00<00:00, 17.8MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/depparse/talbanken_charlm.pt: 100% 135M/135M [00:01<00:00, 87.4MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/ner/suc3shuffle.pt: 100% 65.9M/65.9M [00:00<00:00, 115MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/forward_charlm/conll17.pt: 100% 20.9M/20.9M [00:00<00:00, 65.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/backward_charlm/conll17.pt: 100% 20.9M/20.9M [00:00<00:00, 61.4MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-sv/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:01<00:00, 87.0MB/s]\n",
            "2025-07-18 10:57:11 INFO: Loading these models for language: sv (Swedish):\n",
            "==================================\n",
            "| Processor | Package            |\n",
            "----------------------------------\n",
            "| tokenize  | talbanken          |\n",
            "| pos       | talbanken_charlm   |\n",
            "| lemma     | talbanken_nocharlm |\n",
            "| depparse  | talbanken_charlm   |\n",
            "| ner       | suc3shuffle        |\n",
            "==================================\n",
            "\n",
            "2025-07-18 10:57:11 INFO: Using device: cuda\n",
            "2025-07-18 10:57:11 INFO: Loading: tokenize\n",
            "2025-07-18 10:57:12 INFO: Loading: pos\n",
            "2025-07-18 10:57:14 INFO: Loading: lemma\n",
            "2025-07-18 10:57:14 INFO: Loading: depparse\n",
            "2025-07-18 10:57:14 INFO: Loading: ner\n",
            "2025-07-18 10:57:16 INFO: Done loading processors!\n",
            "2025-07-18 10:57:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.5MB/s]        \n",
            "2025-07-18 10:57:29 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 10:57:30 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 10:57:30 INFO: Using device: cuda\n",
            "2025-07-18 10:57:30 INFO: Loading: tokenize\n",
            "2025-07-18 10:57:30 INFO: Loading: mwt\n",
            "2025-07-18 10:57:30 INFO: Loading: pos\n",
            "2025-07-18 10:57:32 INFO: Loading: lemma\n",
            "2025-07-18 10:57:32 INFO: Loading: constituency\n",
            "2025-07-18 10:57:33 INFO: Loading: depparse\n",
            "2025-07-18 10:57:33 INFO: Loading: sentiment\n",
            "2025-07-18 10:57:34 INFO: Loading: ner\n",
            "2025-07-18 10:57:36 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.bias', 'psi_cls.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear2.weight', 'psi_cls.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 48/48 [00:04<00:00, 10.47it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.bias', 'psi_cls.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'guide_layer.linear.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear2.weight', 'psi_cls.decoder.bias', 'psi_cls.transform.weight', 'psi_cls.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 48/48 [00:04<00:00,  9.84it/s]\n",
            "2025-07-18 10:58:22.123930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752836302.145001   35327 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752836302.151492   35327 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=tr-en\n",
            "test-00000-of-00001.parquet: 100% 154k/154k [00:00<00:00, 4.91MB/s]\n",
            "Generating test split: 553 examples [00:00, 99632.74 examples/s]\n",
            "Working with 1106 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to tr: 100% 35/35 [01:54<00:00,  3.26s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 35/35 [02:16<00:00,  3.89s/it]\n",
            "Parsing\n",
            "2025-07-18 11:03:38 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.5MB/s]        \n",
            "2025-07-18 11:03:38 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/tokenize/imst.pt: 100% 631k/631k [00:00<00:00, 12.5MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/mwt/imst.pt: 100% 611k/611k [00:00<00:00, 12.1MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/pos/imst_charlm.pt: 100% 36.2M/36.2M [00:00<00:00, 207MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/lemma/imst_nocharlm.pt: 100% 2.84M/2.84M [00:00<00:00, 38.2MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/depparse/imst_charlm.pt: 100% 134M/134M [00:00<00:00, 137MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/ner/starlang.pt: 100% 52.8M/52.8M [00:00<00:00, 240MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/pretrain/conll17.pt: 100% 107M/107M [00:00<00:00, 314MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/backward_charlm/conll17.pt: 100% 21.6M/21.6M [00:00<00:00, 171MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-tr/resolve/v1.10.0/models/forward_charlm/conll17.pt: 100% 21.6M/21.6M [00:00<00:00, 172MB/s]\n",
            "2025-07-18 11:03:43 INFO: Loading these models for language: tr (Turkish):\n",
            "=============================\n",
            "| Processor | Package       |\n",
            "-----------------------------\n",
            "| tokenize  | imst          |\n",
            "| mwt       | imst          |\n",
            "| pos       | imst_charlm   |\n",
            "| lemma     | imst_nocharlm |\n",
            "| depparse  | imst_charlm   |\n",
            "| ner       | starlang      |\n",
            "=============================\n",
            "\n",
            "2025-07-18 11:03:43 INFO: Using device: cuda\n",
            "2025-07-18 11:03:43 INFO: Loading: tokenize\n",
            "2025-07-18 11:03:43 INFO: Loading: mwt\n",
            "2025-07-18 11:03:43 INFO: Loading: pos\n",
            "2025-07-18 11:03:45 INFO: Loading: lemma\n",
            "2025-07-18 11:03:45 INFO: Loading: depparse\n",
            "2025-07-18 11:03:46 INFO: Loading: ner\n",
            "2025-07-18 11:03:48 INFO: Done loading processors!\n",
            "2025-07-18 11:03:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 32.0MB/s]        \n",
            "2025-07-18 11:03:58 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 11:04:00 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 11:04:00 INFO: Using device: cuda\n",
            "2025-07-18 11:04:00 INFO: Loading: tokenize\n",
            "2025-07-18 11:04:00 INFO: Loading: mwt\n",
            "2025-07-18 11:04:00 INFO: Loading: pos\n",
            "2025-07-18 11:04:01 INFO: Loading: lemma\n",
            "2025-07-18 11:04:02 INFO: Loading: constituency\n",
            "2025-07-18 11:04:03 INFO: Loading: depparse\n",
            "2025-07-18 11:04:03 INFO: Loading: sentiment\n",
            "2025-07-18 11:04:03 INFO: Loading: ner\n",
            "2025-07-18 11:04:06 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'psi_cls.decoder.bias', 'cls.predictions.bias', 'guide_layer.linear.weight', 'cls.predictions.decoder.weight', 'psi_cls.bias', 'guide_layer.linear2.weight', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 35/35 [00:03<00:00, 10.06it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'psi_cls.decoder.bias', 'cls.predictions.bias', 'guide_layer.linear.weight', 'cls.predictions.decoder.weight', 'psi_cls.bias', 'guide_layer.linear2.weight', 'cls.predictions.transform.LayerNorm.weight', 'guide_layer.linear.bias', 'cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'guide_layer.linear2.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 35/35 [00:04<00:00,  8.73it/s]\n",
            "2025-07-18 11:04:46.144385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752836686.165547   36945 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752836686.172023   36945 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=tr-de\n",
            "test-00000-of-00001.parquet: 100% 89.8k/89.8k [00:00<00:00, 1.82MB/s]\n",
            "Generating test split: 216 examples [00:00, 40606.41 examples/s]\n",
            "Working with 432 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to tr: 100% 14/14 [01:18<00:00,  5.59s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to de: 100% 14/14 [01:28<00:00,  6.30s/it]\n",
            "Parsing\n",
            "2025-07-18 11:08:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 31.8MB/s]        \n",
            "2025-07-18 11:08:39 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 11:08:40 INFO: Loading these models for language: tr (Turkish):\n",
            "=============================\n",
            "| Processor | Package       |\n",
            "-----------------------------\n",
            "| tokenize  | imst          |\n",
            "| mwt       | imst          |\n",
            "| pos       | imst_charlm   |\n",
            "| lemma     | imst_nocharlm |\n",
            "| depparse  | imst_charlm   |\n",
            "| ner       | starlang      |\n",
            "=============================\n",
            "\n",
            "2025-07-18 11:08:40 INFO: Using device: cuda\n",
            "2025-07-18 11:08:40 INFO: Loading: tokenize\n",
            "2025-07-18 11:08:41 INFO: Loading: mwt\n",
            "2025-07-18 11:08:41 INFO: Loading: pos\n",
            "2025-07-18 11:08:43 INFO: Loading: lemma\n",
            "2025-07-18 11:08:43 INFO: Loading: depparse\n",
            "2025-07-18 11:08:43 INFO: Loading: ner\n",
            "2025-07-18 11:08:45 INFO: Done loading processors!\n",
            "2025-07-18 11:08:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.6MB/s]        \n",
            "2025-07-18 11:08:51 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 11:08:53 INFO: Loading these models for language: de (German):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | spmrl_charlm      |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | sb10k_charlm      |\n",
            "| ner          | germeval2014      |\n",
            "====================================\n",
            "\n",
            "2025-07-18 11:08:53 INFO: Using device: cuda\n",
            "2025-07-18 11:08:53 INFO: Loading: tokenize\n",
            "2025-07-18 11:08:53 INFO: Loading: mwt\n",
            "2025-07-18 11:08:53 INFO: Loading: pos\n",
            "2025-07-18 11:08:55 INFO: Loading: lemma\n",
            "2025-07-18 11:09:03 INFO: Loading: constituency\n",
            "2025-07-18 11:09:04 INFO: Loading: depparse\n",
            "2025-07-18 11:09:04 INFO: Loading: sentiment\n",
            "2025-07-18 11:09:05 INFO: Loading: ner\n",
            "2025-07-18 11:09:08 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.weight', 'guide_layer.linear2.bias', 'guide_layer.linear.bias', 'guide_layer.linear2.weight', 'cls.predictions.bias', 'psi_cls.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 14/14 [00:01<00:00,  7.23it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'psi_cls.transform.weight', 'guide_layer.linear2.bias', 'guide_layer.linear.bias', 'guide_layer.linear2.weight', 'cls.predictions.bias', 'psi_cls.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.bias', 'psi_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 14/14 [00:02<00:00,  6.86it/s]\n",
            "2025-07-18 11:09:31.726046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752836971.747293   38159 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752836971.754168   38159 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "split=test, lang=zh-en\n",
            "test-00000-of-00001.parquet: 100% 60.1k/60.1k [00:00<00:00, 5.21MB/s]\n",
            "Generating test split: 201 examples [00:00, 40978.72 examples/s]\n",
            "Working with 402 samples\n",
            "Translating\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to zh: 100% 13/13 [00:25<00:00,  1.96s/it]\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at jbochi/madlad400-3b-mt and are newly initialized: ['shared.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Madlad translate to en: 100% 13/13 [00:30<00:00,  2.34s/it]\n",
            "Parsing\n",
            "2025-07-18 11:11:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 30.7MB/s]        \n",
            "2025-07-18 11:11:34 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/tokenize/gsdsimp.pt: 100% 1.38M/1.38M [00:00<00:00, 7.46MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/pos/gsdsimp_charlm.pt: 100% 35.0M/35.0M [00:00<00:00, 72.8MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/lemma/gsdsimp_nocharlm.pt: 100% 6.59M/6.59M [00:00<00:00, 24.6MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/constituency/ctb-51_charlm.pt: 100% 124M/124M [00:01<00:00, 113MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/depparse/gsdsimp_charlm.pt: 100% 136M/136M [00:02<00:00, 60.0MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/sentiment/ren_charlm.pt: 100% 118M/118M [00:01<00:00, 106MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/ner/ontonotes.pt: 100% 103M/103M [00:01<00:00, 102MB/s] \n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/forward_charlm/gigaword.pt: 100% 43.1M/43.1M [00:00<00:00, 79.7MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/pretrain/fasttext157.pt: 100% 307M/307M [00:02<00:00, 125MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hans/resolve/v1.10.0/models/backward_charlm/gigaword.pt: 100% 43.1M/43.1M [00:00<00:00, 84.9MB/s]\n",
            "2025-07-18 11:11:48 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
            "===================================\n",
            "| Processor    | Package          |\n",
            "-----------------------------------\n",
            "| tokenize     | gsdsimp          |\n",
            "| pos          | gsdsimp_charlm   |\n",
            "| lemma        | gsdsimp_nocharlm |\n",
            "| constituency | ctb-51_charlm    |\n",
            "| depparse     | gsdsimp_charlm   |\n",
            "| sentiment    | ren_charlm       |\n",
            "| ner          | ontonotes        |\n",
            "===================================\n",
            "\n",
            "2025-07-18 11:11:48 INFO: Using device: cuda\n",
            "2025-07-18 11:11:48 INFO: Loading: tokenize\n",
            "2025-07-18 11:11:49 INFO: Loading: pos\n",
            "2025-07-18 11:11:51 INFO: Loading: lemma\n",
            "2025-07-18 11:11:51 INFO: Loading: constituency\n",
            "2025-07-18 11:11:52 INFO: Loading: depparse\n",
            "2025-07-18 11:11:53 INFO: Loading: sentiment\n",
            "2025-07-18 11:11:53 INFO: Loading: ner\n",
            "2025-07-18 11:11:56 INFO: Done loading processors!\n",
            "2025-07-18 11:12:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 433kB [00:00, 29.9MB/s]        \n",
            "2025-07-18 11:12:02 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
            "2025-07-18 11:12:04 INFO: Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "2025-07-18 11:12:04 INFO: Using device: cuda\n",
            "2025-07-18 11:12:04 INFO: Loading: tokenize\n",
            "2025-07-18 11:12:04 INFO: Loading: mwt\n",
            "2025-07-18 11:12:04 INFO: Loading: pos\n",
            "2025-07-18 11:12:05 INFO: Loading: lemma\n",
            "2025-07-18 11:12:06 INFO: Loading: constituency\n",
            "2025-07-18 11:12:07 INFO: Loading: depparse\n",
            "2025-07-18 11:12:07 INFO: Loading: sentiment\n",
            "2025-07-18 11:12:07 INFO: Loading: ner\n",
            "2025-07-18 11:12:10 INFO: Done loading processors!\n",
            "Aligning\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'psi_cls.decoder.weight', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.weight', 'guide_layer.linear.bias', 'psi_cls.transform.bias', 'guide_layer.linear2.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.bias', 'guide_layer.linear2.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 13/13 [00:01<00:00, 12.02it/s]\n",
            "Some weights of the model checkpoint at aneuraz/awesome-align-with-co were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'guide_layer.linear.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'psi_cls.decoder.weight', 'psi_cls.decoder.bias', 'cls.predictions.decoder.bias', 'psi_cls.transform.weight', 'guide_layer.linear.bias', 'psi_cls.transform.bias', 'guide_layer.linear2.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'psi_cls.bias', 'guide_layer.linear2.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at aneuraz/awesome-align-with-co and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 13/13 [00:01<00:00, 11.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=$(pwd) python csntax/train.py\n"
      ],
      "metadata": {
        "id": "4uzpDICJMR8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db62feb-003f-4d3f-edc1-5899d9723e04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: 18\n",
            "Dep Types: 37\n",
            "100% 7501/7501 [00:33<00:00, 221.16it/s]\n",
            "100% 1932/1932 [00:08<00:00, 220.37it/s]\n",
            "Train: 7501\n",
            "Val: 1932\n",
            "Setting seed: 77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33migorsterner\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/csntax-gnn/wandb/run-20250718_111715-s8i0xcm7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrue-cherry-9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/s8i0xcm7\u001b[0m\n",
            "100% 100/100 [02:26<00:00,  1.46s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss ▆█▄▆▅▅▇▅▅▆▄▅▃▂▃▃▂▅▄▂▂▁▅▄▃▂▅▅▅▁▃▃▅▃▄▅▂▁▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc ▁▂▂▂▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc ▁▂▃▂▃▃▄▄▃▅▆▆▆▇▇▇▇▇▇▇█▇█▇▇██▇▇▇█▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss 0.455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc 0.86482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc 0.81004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgine (seed-77)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/s8i0xcm7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250718_111715-s8i0xcm7/logs\u001b[0m\n",
            "Setting seed: 123\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/csntax-gnn/wandb/run-20250718_111942-evgczp7h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-butterfly-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/evgczp7h\u001b[0m\n",
            "100% 100/100 [02:25<00:00,  1.45s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss ███▅▄▄▅▃▄▃▂▃▃▃▄▂▄▄▂▃▃▃▂▃▂▂▂▂▃▃▂▂▂▁▄▂▁▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc ▁▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc ▁▂▃▂▃▅▇▇▇▇▇▇▇▇▇▇▇▇▇██▆▇▇█▇█▇██▇███▇▇██▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss 0.42925\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc 0.86402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc 0.79917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgine (seed-123)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/evgczp7h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250718_111942-evgczp7h/logs\u001b[0m\n",
            "Setting seed: 253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/csntax-gnn/wandb/run-20250718_112209-c7cgxjd3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-monkey-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/c7cgxjd3\u001b[0m\n",
            "100% 100/100 [02:25<00:00,  1.45s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss ██▅▆▅▄▄▃▃▄▃▃▄▃▃▃▃▃▄▄▄▃▃▃▃▁▃▄▂▃▃▂▃▄▃▃▃▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc ▁▁▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc ▁▄▅▆▆▆▇▇▇█▇▇█████████▇██████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/loss 0.37671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_acc 0.87002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val/val_acc 0.80642\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgine (seed-253)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025/runs/c7cgxjd3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/igorsterner/ACL2025\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250718_112209-c7cgxjd3/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=$(pwd) python csntax/inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm2dTYw-2uJm",
        "outputId": "4eae2eee-4169-4afe-bdaf-99720a5de236"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de-en\n",
            "100% 1000/1000 [00:04<00:00, 226.52it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 79.7%\n",
            "(797 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 217.28it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 78.0%\n",
            "(780 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 227.30it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 81.2%\n",
            "(812 out of 1000 correct)\n",
            "da-en\n",
            "100% 330/330 [00:01<00:00, 224.79it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 72.7%\n",
            "(240 out of 330 correct)\n",
            "100% 330/330 [00:01<00:00, 259.23it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 70.3%\n",
            "(232 out of 330 correct)\n",
            "100% 330/330 [00:01<00:00, 260.39it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 71.2%\n",
            "(235 out of 330 correct)\n",
            "es-en\n",
            "100% 860/860 [00:03<00:00, 246.45it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 72.3%\n",
            "(622 out of 860 correct)\n",
            "100% 860/860 [00:03<00:00, 248.94it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 69.1%\n",
            "(594 out of 860 correct)\n",
            "100% 860/860 [00:03<00:00, 233.91it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 72.8%\n",
            "(626 out of 860 correct)\n",
            "fr-en\n",
            "100% 1000/1000 [00:03<00:00, 272.39it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 75.7%\n",
            "(757 out of 1000 correct)\n",
            "100% 1000/1000 [00:03<00:00, 271.22it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 74.7%\n",
            "(747 out of 1000 correct)\n",
            "100% 1000/1000 [00:03<00:00, 255.44it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 76.0%\n",
            "(760 out of 1000 correct)\n",
            "it-en\n",
            "100% 1000/1000 [00:04<00:00, 242.66it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 76.1%\n",
            "(761 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 242.58it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 74.1%\n",
            "(741 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 228.38it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 77.0%\n",
            "(770 out of 1000 correct)\n",
            "id-en\n",
            "100% 1000/1000 [00:04<00:00, 243.82it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 74.9%\n",
            "(749 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 244.94it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 71.3%\n",
            "(713 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 244.33it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 75.7%\n",
            "(757 out of 1000 correct)\n",
            "nl-en\n",
            "100% 1000/1000 [00:04<00:00, 229.54it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 75.2%\n",
            "(752 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 241.43it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 75.0%\n",
            "(750 out of 1000 correct)\n",
            "100% 1000/1000 [00:04<00:00, 242.63it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 74.8%\n",
            "(748 out of 1000 correct)\n",
            "sv-en\n",
            "100% 760/760 [00:03<00:00, 237.08it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 72.4%\n",
            "(550 out of 760 correct)\n",
            "100% 760/760 [00:03<00:00, 238.36it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 71.4%\n",
            "(543 out of 760 correct)\n",
            "100% 760/760 [00:03<00:00, 238.18it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 73.6%\n",
            "(559 out of 760 correct)\n",
            "tr-en\n",
            "100% 553/553 [00:02<00:00, 242.63it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 66.9%\n",
            "(370 out of 553 correct)\n",
            "100% 553/553 [00:02<00:00, 248.36it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 72.2%\n",
            "(399 out of 553 correct)\n",
            "100% 553/553 [00:02<00:00, 248.10it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 68.2%\n",
            "(377 out of 553 correct)\n",
            "tr-de\n",
            "100% 216/216 [00:01<00:00, 148.83it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 62.5%\n",
            "(135 out of 216 correct)\n",
            "100% 216/216 [00:01<00:00, 153.24it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 63.4%\n",
            "(137 out of 216 correct)\n",
            "100% 216/216 [00:01<00:00, 151.28it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 62.5%\n",
            "(135 out of 216 correct)\n",
            "zh-en\n",
            "100% 201/201 [00:00<00:00, 257.55it/s]\n",
            "Setting seed: 77\n",
            "Test Accuracy: 61.2%\n",
            "(123 out of 201 correct)\n",
            "100% 201/201 [00:00<00:00, 259.84it/s]\n",
            "Setting seed: 123\n",
            "Test Accuracy: 64.2%\n",
            "(129 out of 201 correct)\n",
            "100% 201/201 [00:00<00:00, 213.00it/s]\n",
            "Setting seed: 253\n",
            "Test Accuracy: 62.7%\n",
            "(126 out of 201 correct)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTHONPATH=$(pwd) python csntax/baseline/run.py --seed 77 --model_name xlm-roberta-base\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmTvJXg6SdDW",
        "outputId": "655f077a-7e1c-423a-bfec-1fb8cb960c2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting seed: 77\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 21.4MB/s]\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 218kB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 4.83MB/s]\n",
            "model.safetensors: 100% 1.12G/1.12G [00:03<00:00, 343MB/s]\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100% 59/59 [00:29<00:00,  2.00it/s]\n",
            "Epoch 1/80: train A=0.4981\n",
            "100% 12/12 [00:00<00:00, 409200.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 2/80: train A=0.5071\n",
            "100% 12/12 [00:00<00:00, 372827.02it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 3/80: train A=0.5115\n",
            "100% 12/12 [00:00<00:00, 344737.32it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 4/80: train A=0.5087\n",
            "100% 12/12 [00:00<00:00, 415964.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 5/80: train A=0.5135\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 6/80: train A=0.5058\n",
            "100% 12/12 [00:00<00:00, 405900.39it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 7/80: train A=0.5205\n",
            "100% 12/12 [00:00<00:00, 426539.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 8/80: train A=0.5143\n",
            "100% 12/12 [00:00<00:00, 433893.52it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 9/80: train A=0.5189\n",
            "100% 12/12 [00:00<00:00, 445412.81it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 10/80: train A=0.5203\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 10/80: val A=id-en_test.json): 0.5110\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 10/80: val A=zh-en_test.json): 0.5124\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 10/80: val A=it-en_test.json): 0.4760\n",
            " 25% 3/12 [00:02<00:09,  1.01s/it]Epoch 10/80: val A=de-en_validation.json): 0.5050\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 10/80: val A=sv-en_test.json): 0.5013\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 10/80: val A=es-en_test.json): 0.5174\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 10/80: val A=da-en_test.json): 0.4667\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 10/80: val A=de-en_test.json): 0.5020\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 10/80: val A=tr-de_test.json): 0.5139\n",
            " 75% 9/12 [00:08<00:02,  1.22it/s]Epoch 10/80: val A=nl-en_test.json): 0.4740\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 10/80: val A=tr-en_test.json): 0.4756\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 10/80: val A=fr-en_test.json): 0.5330\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 11/80: train A=0.5318\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 12/80: train A=0.5223\n",
            "100% 12/12 [00:00<00:00, 402653.18it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 13/80: train A=0.5258\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 14/80: train A=0.5197\n",
            "100% 12/12 [00:00<00:00, 433893.52it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 15/80: train A=0.5241\n",
            "100% 12/12 [00:00<00:00, 246723.76it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 16/80: train A=0.5090\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 17/80: train A=0.5318\n",
            "100% 12/12 [00:00<00:00, 445412.81it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 18/80: train A=0.5250\n",
            "100% 12/12 [00:00<00:00, 441505.68it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 19/80: train A=0.5399\n",
            "100% 12/12 [00:00<00:00, 419430.40it/s]\n",
            "100% 59/59 [00:28<00:00,  2.03it/s]\n",
            "Epoch 20/80: train A=0.5214\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 20/80: val A=id-en_test.json): 0.5610\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 20/80: val A=zh-en_test.json): 0.5572\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 20/80: val A=it-en_test.json): 0.5900\n",
            " 25% 3/12 [00:02<00:09,  1.00s/it]Epoch 20/80: val A=de-en_validation.json): 0.6050\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 20/80: val A=sv-en_test.json): 0.5408\n",
            " 42% 5/12 [00:05<00:07,  1.11s/it]Epoch 20/80: val A=es-en_test.json): 0.5616\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 20/80: val A=da-en_test.json): 0.5212\n",
            " 58% 7/12 [00:06<00:04,  1.14it/s]Epoch 20/80: val A=de-en_test.json): 0.5680\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 20/80: val A=tr-de_test.json): 0.5972\n",
            " 75% 9/12 [00:08<00:02,  1.23it/s]Epoch 20/80: val A=nl-en_test.json): 0.5420\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 20/80: val A=tr-en_test.json): 0.5063\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 20/80: val A=fr-en_test.json): 0.5490\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 21/80: train A=0.5325\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 22/80: train A=0.5294\n",
            "100% 12/12 [00:00<00:00, 402653.18it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 23/80: train A=0.5361\n",
            "100% 12/12 [00:00<00:00, 426539.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 24/80: train A=0.5365\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 25/80: train A=0.5521\n",
            "100% 12/12 [00:00<00:00, 433893.52it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 26/80: train A=0.5398\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 27/80: train A=0.5441\n",
            "100% 12/12 [00:00<00:00, 297820.40it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 28/80: train A=0.5470\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 29/80: train A=0.5550\n",
            "100% 12/12 [00:00<00:00, 412554.49it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 30/80: train A=0.5385\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 30/80: val A=id-en_test.json): 0.5060\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 30/80: val A=zh-en_test.json): 0.4975\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 30/80: val A=it-en_test.json): 0.4750\n",
            " 25% 3/12 [00:02<00:08,  1.00it/s]Epoch 30/80: val A=de-en_validation.json): 0.5300\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 30/80: val A=sv-en_test.json): 0.5066\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 30/80: val A=es-en_test.json): 0.5116\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 30/80: val A=da-en_test.json): 0.4727\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 30/80: val A=de-en_test.json): 0.5130\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 30/80: val A=tr-de_test.json): 0.4907\n",
            " 75% 9/12 [00:08<00:02,  1.22it/s]Epoch 30/80: val A=nl-en_test.json): 0.4990\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 30/80: val A=tr-en_test.json): 0.4919\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 30/80: val A=fr-en_test.json): 0.4910\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 31/80: train A=0.5394\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 32/80: train A=0.5331\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 33/80: train A=0.5495\n",
            "100% 12/12 [00:00<00:00, 461758.24it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 34/80: train A=0.5173\n",
            "100% 12/12 [00:00<00:00, 402653.18it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 35/80: train A=0.5413\n",
            "100% 12/12 [00:00<00:00, 249166.57it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 36/80: train A=0.5523\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 37/80: train A=0.5401\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 38/80: train A=0.5419\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 39/80: train A=0.5529\n",
            "100% 12/12 [00:00<00:00, 433893.52it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 40/80: train A=0.5391\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 40/80: val A=id-en_test.json): 0.5390\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 40/80: val A=zh-en_test.json): 0.5025\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 40/80: val A=it-en_test.json): 0.5740\n",
            " 25% 3/12 [00:02<00:09,  1.00s/it]Epoch 40/80: val A=de-en_validation.json): 0.5210\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 40/80: val A=sv-en_test.json): 0.5092\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 40/80: val A=es-en_test.json): 0.5267\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 40/80: val A=da-en_test.json): 0.4667\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 40/80: val A=de-en_test.json): 0.5450\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 40/80: val A=tr-de_test.json): 0.5231\n",
            " 75% 9/12 [00:08<00:02,  1.22it/s]Epoch 40/80: val A=nl-en_test.json): 0.4850\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 40/80: val A=tr-en_test.json): 0.5353\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 40/80: val A=fr-en_test.json): 0.5350\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 41/80: train A=0.5507\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 42/80: train A=0.5383\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 43/80: train A=0.5543\n",
            "100% 12/12 [00:00<00:00, 384211.05it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 44/80: train A=0.5526\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 45/80: train A=0.5571\n",
            "100% 12/12 [00:00<00:00, 445412.81it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 46/80: train A=0.5513\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 47/80: train A=0.5542\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 48/80: train A=0.5345\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 49/80: train A=0.5541\n",
            "100% 12/12 [00:00<00:00, 426539.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 50/80: train A=0.5570\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 50/80: val A=id-en_test.json): 0.5370\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 50/80: val A=zh-en_test.json): 0.5970\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 50/80: val A=it-en_test.json): 0.5690\n",
            " 25% 3/12 [00:02<00:08,  1.00it/s]Epoch 50/80: val A=de-en_validation.json): 0.5810\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 50/80: val A=sv-en_test.json): 0.5303\n",
            " 42% 5/12 [00:05<00:07,  1.11s/it]Epoch 50/80: val A=es-en_test.json): 0.5570\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 50/80: val A=da-en_test.json): 0.4848\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 50/80: val A=de-en_test.json): 0.5630\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 50/80: val A=tr-de_test.json): 0.5648\n",
            " 75% 9/12 [00:08<00:02,  1.23it/s]Epoch 50/80: val A=nl-en_test.json): 0.5040\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 50/80: val A=tr-en_test.json): 0.5497\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 50/80: val A=fr-en_test.json): 0.5360\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 51/80: train A=0.5606\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 52/80: train A=0.5498\n",
            "100% 12/12 [00:00<00:00, 445412.81it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 53/80: train A=0.5418\n",
            "100% 12/12 [00:00<00:00, 316551.25it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 54/80: train A=0.5459\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 55/80: train A=0.5566\n",
            "100% 12/12 [00:00<00:00, 310689.19it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 56/80: train A=0.5587\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 57/80: train A=0.5610\n",
            "100% 12/12 [00:00<00:00, 415964.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 58/80: train A=0.5563\n",
            "100% 12/12 [00:00<00:00, 297820.40it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 59/80: train A=0.5653\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 60/80: train A=0.5602\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 60/80: val A=id-en_test.json): 0.5540\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 60/80: val A=zh-en_test.json): 0.5672\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 60/80: val A=it-en_test.json): 0.5600\n",
            " 25% 3/12 [00:02<00:08,  1.00it/s]Epoch 60/80: val A=de-en_validation.json): 0.5690\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 60/80: val A=sv-en_test.json): 0.5566\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 60/80: val A=es-en_test.json): 0.5709\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 60/80: val A=da-en_test.json): 0.5091\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 60/80: val A=de-en_test.json): 0.5520\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 60/80: val A=tr-de_test.json): 0.5093\n",
            " 75% 9/12 [00:08<00:02,  1.22it/s]Epoch 60/80: val A=nl-en_test.json): 0.5420\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 60/80: val A=tr-en_test.json): 0.4919\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 60/80: val A=fr-en_test.json): 0.5460\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 61/80: train A=0.5462\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 62/80: train A=0.5471\n",
            "100% 12/12 [00:00<00:00, 445412.81it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 63/80: train A=0.5626\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 64/80: train A=0.5661\n",
            "100% 12/12 [00:00<00:00, 320583.75it/s]\n",
            "100% 59/59 [00:29<00:00,  2.03it/s]\n",
            "Epoch 65/80: train A=0.5650\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 66/80: train A=0.5546\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:29<00:00,  2.02it/s]\n",
            "Epoch 67/80: train A=0.5621\n",
            "100% 12/12 [00:00<00:00, 449389.71it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 68/80: train A=0.5635\n",
            "100% 12/12 [00:00<00:00, 324720.31it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 69/80: train A=0.5571\n",
            "100% 12/12 [00:00<00:00, 422955.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 70/80: train A=0.5539\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 70/80: val A=id-en_test.json): 0.5550\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 70/80: val A=zh-en_test.json): 0.5970\n",
            " 17% 2/12 [00:01<00:07,  1.39it/s]Epoch 70/80: val A=it-en_test.json): 0.5620\n",
            " 25% 3/12 [00:02<00:09,  1.00s/it]Epoch 70/80: val A=de-en_validation.json): 0.5580\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 70/80: val A=sv-en_test.json): 0.5289\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 70/80: val A=es-en_test.json): 0.5430\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 70/80: val A=da-en_test.json): 0.4939\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 70/80: val A=de-en_test.json): 0.5540\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 70/80: val A=tr-de_test.json): 0.5463\n",
            " 75% 9/12 [00:08<00:02,  1.23it/s]Epoch 70/80: val A=nl-en_test.json): 0.5280\n",
            " 83% 10/12 [00:10<00:01,  1.00it/s]Epoch 70/80: val A=tr-en_test.json): 0.5280\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 70/80: val A=fr-en_test.json): 0.5520\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 71/80: train A=0.5579\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 72/80: train A=0.5505\n",
            "100% 12/12 [00:00<00:00, 437666.50it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 73/80: train A=0.5643\n",
            "100% 12/12 [00:00<00:00, 433893.52it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 74/80: train A=0.5635\n",
            "100% 12/12 [00:00<00:00, 426539.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.06it/s]\n",
            "Epoch 75/80: train A=0.5699\n",
            "100% 12/12 [00:00<00:00, 441505.68it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 76/80: train A=0.5658\n",
            "100% 12/12 [00:00<00:00, 430185.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 77/80: train A=0.5631\n",
            "100% 12/12 [00:00<00:00, 415964.03it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 78/80: train A=0.5747\n",
            "100% 12/12 [00:00<00:00, 441505.68it/s]\n",
            "100% 59/59 [00:28<00:00,  2.05it/s]\n",
            "Epoch 79/80: train A=0.5693\n",
            "100% 12/12 [00:00<00:00, 426539.39it/s]\n",
            "100% 59/59 [00:28<00:00,  2.04it/s]\n",
            "Epoch 80/80: train A=0.5773\n",
            "  0% 0/12 [00:00<?, ?it/s]Epoch 80/80: val A=id-en_test.json): 0.5660\n",
            "  8% 1/12 [00:01<00:15,  1.44s/it]Epoch 80/80: val A=zh-en_test.json): 0.5224\n",
            " 17% 2/12 [00:01<00:07,  1.40it/s]Epoch 80/80: val A=it-en_test.json): 0.5700\n",
            " 25% 3/12 [00:02<00:08,  1.00it/s]Epoch 80/80: val A=de-en_validation.json): 0.5920\n",
            " 33% 4/12 [00:04<00:09,  1.14s/it]Epoch 80/80: val A=sv-en_test.json): 0.5303\n",
            " 42% 5/12 [00:05<00:07,  1.12s/it]Epoch 80/80: val A=es-en_test.json): 0.5337\n",
            " 50% 6/12 [00:06<00:06,  1.12s/it]Epoch 80/80: val A=da-en_test.json): 0.5515\n",
            " 58% 7/12 [00:06<00:04,  1.13it/s]Epoch 80/80: val A=de-en_test.json): 0.5860\n",
            " 67% 8/12 [00:08<00:04,  1.03s/it]Epoch 80/80: val A=tr-de_test.json): 0.5046\n",
            " 75% 9/12 [00:08<00:02,  1.22it/s]Epoch 80/80: val A=nl-en_test.json): 0.5220\n",
            " 83% 10/12 [00:10<00:02,  1.00s/it]Epoch 80/80: val A=tr-en_test.json): 0.5118\n",
            " 92% 11/12 [00:10<00:00,  1.08it/s]Epoch 80/80: val A=fr-en_test.json): 0.5290\n",
            "100% 12/12 [00:12<00:00,  1.01s/it]\n"
          ]
        }
      ]
    }
  ]
}